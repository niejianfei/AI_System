{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 微分基本概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微分：符号微分，数值微分，自动微分\n",
    "# 自动微分：表达式追踪（evaluation trace）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 = x\n",
    "# L(n+1) = 4L(n) * (1 - L(n))\n",
    "# (v, dv) = (x, 1) # 初代\n",
    "# for i in range(3):\n",
    "#     (v, dv) = ((4*v)*(1-v), 4*dv-8*v*dv)  # 微分表达式迭代后不变\n",
    "# (v,dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 自动微分的两种模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动微分：前向微分，后向微分\n",
    "# 原理：所有的数值计算都有基本运算组成，基本运算的导数表达式已知，通过链式法则将数值运算各部分组合成整体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.原函数转化为DAG图；2.根据链式求导法则展开\n",
    "# 正向求导：求链路上所有节点对xi的导数，一次只能对某一个变量求导；\n",
    "# 反向求导：求DAG所有节点的导数，一次可以对所有变量求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求解jacobian矩阵，向量对向量求导\n",
    "# 正向模式，jacobian-vector production\n",
    "# 反向模式，vector-jacobian production\n",
    "# 对于一个输出变量yi进行一次反向模式，得到jacobian矩阵的1行，dyi/dx1，...，dyi/dxn\n",
    "# 对于一个输入变量xi进行一次正向模式，得到jacobian矩阵的一列，dy1/dxi,...,dym/dxi\n",
    "# 当m>n适合用正向模型；当x<n适合用反向模式  神经网络！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 自动微分实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表达式或图(库函数)，操作符重载OO（pytorch，tensorflow），源码转换AST（mindspore）\n",
    "# 基于表达式实现主要依赖构建基础微分表达库，手动调用库\n",
    "# 基于操作符重载依赖于语言的多态性来记录实现\n",
    "# 基于源码转换核心在于AST完成基本表达式的分解和微分操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 实现正向自动微分的AI框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分解程序为一系列已知微分规则的基础表达式组合，并使用高级语言的重载操作\n",
    "# 在重载运算操作的过程中，根据已知微分规则给出各基础表达式的微分结果\n",
    "# 根据基础表达式间的数据依赖关系，使用链式法则将微分结果组合完成程序的微分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADTangent:\n",
    "    def __init__(self, x, dx):\n",
    "        self.x = x\n",
    "        self.dx = dx\n",
    "    \n",
    "    # print ADTangent对象时会输出str\n",
    "    def __str__(self):\n",
    "        content = f\"value:{self.x:.4f}, grad:{self.dx}\"\n",
    "        return content\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x + other.x\n",
    "            dx = self.dx + other.dx\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x + other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x - other.x\n",
    "            dx = self.dx - other.dx\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x - other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x * other.x\n",
    "            dx = self.dx * other.x + self.x * other.dx\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x * other\n",
    "            dx = self.dx * other\n",
    "        else:\n",
    "            NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    # 对自身的操作，涉及一个对象\n",
    "    def log(self):\n",
    "        x = np.log(self.x)\n",
    "        dx = 1 / self.x * self.dx\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def sin(self):\n",
    "        x = np.sin(self.x)\n",
    "        dx = np.cos(self.x) * self.dx\n",
    "        return ADTangent(x, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value:11.6521, grad:5.5\n"
     ]
    }
   ],
   "source": [
    "# 使用ADTangent重载操作符\n",
    "# sin和log是静态方法可以这样写\n",
    "x1 = ADTangent(2, 1)\n",
    "x2 = ADTangent(5, 0)\n",
    "f = ADTangent.log(x1) + x1 * x2 - ADTangent.sin(x2)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value:11.6521, grad:5.5\n"
     ]
    }
   ],
   "source": [
    "# 正解，sin和log不是静态方法\n",
    "f = x1.log() + x1 * x2 - x2.sin()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11.6521, grad_fn=<SubBackward0>), tensor(5.5000), tensor(1.7163))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor(2.0, requires_grad=True)\n",
    "x2 = torch.tensor(5.0, requires_grad=True)\n",
    "f = torch.log(x1) + x1 * x2 - torch.sin(x2)\n",
    "f.backward()\n",
    "f, x1.grad, x2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 亲自实现一个pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多态性是继封装性和继承性之后，面向对象的第三大特性。\n",
    "# 它是指在父类中定义的属性和方法被子类继承之后，可以具有不同的数据类型或表现出不同的行为，\n",
    "# 这使得同一个属性或方法在父类及其各个子类中具有不同的含义。\n",
    "# 对面向对象来说，多态分为编译时多态和运行时多态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其中编译时多态是静态的，主要是指方法的重载，它是根据参数列表的不同来区分不同的方法。\n",
    "# 通过编译之后会变成两个不同的方法，在运行时谈不上多态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Java的引用变量有两个类型，等号左边类型称为编译时类型，等号右边类型称为运行时类型。\n",
    "# 编译时类型：声明引用变量的类型。\n",
    "# 运行时类型：实际赋给引用变量的类型。\n",
    "# 当编译时类型和运行时类型不一致时，就产生了对象的多态性。\n",
    "# 可以将子类的向上转型看作是基本类型的自动类型转换，\n",
    "# 当子类向上转型为父类后，其引用类型就是父类类型。\n",
    "# 通过父类的引用是无法访问到子类对象中特有的属性和方法，只能访问父类中存在的属性和方法。\n",
    "# 使用多态的方式调用父类中存在的方法时，实际上调用的是子类覆盖重写后的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前AI框架中使用操作符重载 00 的一个典型代表是 Pytroch，其中使用数据结构 Tape来记录计算流程\n",
    "# 在反向模式求解梯度的过程中进行 replay Operator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 操作符重载:预定义了特定的数据结构，并对该数据结构重载了相应的基本运算操作符\n",
    "# Tape记录: 程序在实际执行时会将相应表达式的操作类型和输入输出信息记录至特殊数据结构\n",
    "# 遍历微分:得到特殊数据结构后，将对数据结构进行遍历并对其中记录的基本运算操作进行微分\n",
    "# 链式组合:把结果通过链式法则进行组合，完成自动微分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
